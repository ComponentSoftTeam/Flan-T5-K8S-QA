# KubWizzard
Fine-tuning an instruction-tuned Mistral 7B model with instruction-code pairs, specifically with kubectl commands.

## Dataset
We used a dataset generated by the subproject `kubeget` that scrapes the official documentation for kubectl and augments the gathered data using the openai GPT APIs.  
You can find the dataset on huggingface [here](https://huggingface.co/datasets/ComponentSoft/k8s-kubectl-cot-20k)

## Notebooks
The finetuning process of the base model is documented in this notebook

| Target          | Notebook                                       |
| --------------- | ---------------------------------------------- |
| Kubectl    | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ComponentSoftTeam/Mistral-Kubectl-Instruct/blob/main/notebooks/q-lora-mistra-7b.ipynb) |
